{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a37558da-35c8-4fe2-8acd-74659c658791",
   "metadata": {},
   "source": [
    "# Naive Bayes en Scikit Learn\n",
    "\n",
    "En este _notebook_ vamos a aprender a trabajar con el modelo **Multinomial Naive Bayes** en Scikit Learn. Lo primero que haremos será importar el _dataset_ con el que vamos a trabajar. Corresponde a un _dataset_ de correos, en el que hay correos deseados (fáciles de detectar), correos deseados (difíciles de detectar) y correos no deseados (_spam_). Para cargar los correos pasamos el _path_ de la carpeta `mails-raw` como se ve más abajo, que es la carpeta que encontrarás junto a este _notebook_ y que contiene los correos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30f1e319-686b-4ad8-a08b-2b02fb5f1c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "PATH_MAILS = 'mails-raw/*/*'\n",
    "\n",
    "mails = []\n",
    "answers = []\n",
    "\n",
    "for filename in glob.glob(PATH_MAILS):\n",
    "    # Checkeamos si es spam o no\n",
    "    is_spam = 'ham' not in filename\n",
    "    \n",
    "    # Ignoramos los posibles errores al abrir un archivo\n",
    "    with open(filename, errors='ignore') as mail_file:\n",
    "        for line in mail_file:\n",
    "            if line.startswith(\"Subject:\"):\n",
    "                # Hacemos strip desde la izquierda\n",
    "                subject = line.lstrip(\"Subject: \")\n",
    "                mails.append(subject.strip())\n",
    "                if is_spam:\n",
    "                    answers.append(1)\n",
    "                else:\n",
    "                    answers.append(0)\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2642a577",
   "metadata": {},
   "source": [
    "Vamos a explorar algunos correos. Notemos que estamos trabajando solamente con el asuunto del correo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5adf135b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Friend, Copy ANY DVD or Playstation Game with this software......',\n",
       " '5% Guaranteed for Eight Years',\n",
       " 'Congratulations! You Get a Free Handheld Organizer!',\n",
       " 'One of a kind Money maker! Try it for free!',\n",
       " 'Online Doctors will fill your Viagra Prescription Now!!!                QEEB',\n",
       " 'Take your Marketing to the Next Level',\n",
       " 'One Sale - Three Commission Streams',\n",
       " 'Find Peace, Harmony, Tranquility, And Happiness Right Now!',\n",
       " 'ADV: Extended Auto Warranties Here                                                    undoc',\n",
       " 'Definitely the answer many have been waiting for!!']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mails[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056c80d0",
   "metadata": {},
   "source": [
    "Y vamos a ver la respuesta a estas instancias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "805010bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97104b2-49e7-434d-9d91-57fb0e346c80",
   "metadata": {},
   "source": [
    "Si bien en clases vimos cómo construir un clasificador _Naive Bayes_, en la práctica este funciona un poco distinto. Lo más importante es que en la fase de entrenamiento este clasificador espera recibir un _dataset_ pre-procesado, que es una matriz que tiene un correo en cada fila y en la columna tiene todo nuestro vocabulario observado de palabras. En la posición $(i,j)$ (fila, columna) tenemos el número de veces que la palabra $j$ aparece en el correo $i$. Este procesamiento lo podemos hacer rápidamente con el objeto `CountVectorizer`.\n",
    "\n",
    "Primero, vamos a unir el _dataset_ y separarlo en una parte de **entrenamiento** y otra de **prueba**. Luego armamos el vocabulario con la clase `CountVectorizer` sobre el _dataset_ de entrenamiento. **Ojo**: es importante que al momento de apicar esta transformación al _dataset_, solo lo hagamos con los datos de entrenamiento, ya que si incluimos los datos de prueba, podemos estar filtrando información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff3cb72a-6680-466e-b4cc-1db55f0452a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Recordemos que la función train_test_split produce un sampleo\n",
    "# representativo de las clases\n",
    "X_train, X_test, y_train, y_test = train_test_split(mails, answers)\n",
    "\n",
    "# Vamos a entrenar un CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "vect.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc66a6d0-2c14-486e-801b-77f140726c70",
   "metadata": {},
   "source": [
    "Como vemos, solo preparamos el objeto de tipo `CountVectorizer`. Ahora, lo que vamos a hacer, es transformar los _dataset_ de entrenamiento y prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c118f845-5cb8-4bb1-95d2-2ce3ac066e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vect = vect.transform(X_train)\n",
    "# Notar que estamos transformando con el fit del dataset de entrenamiento\n",
    "# No debemos re-entrenar con los datos de test\n",
    "# Si una palabra no es conocida por el CountVectorizer simplemente será ignorada\n",
    "X_test_vect = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e44ecb",
   "metadata": {},
   "source": [
    "Ahora vamos a explorar brevemente el _dataset_ de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "425326d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2475x3763 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 14316 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3706b47",
   "metadata": {},
   "source": [
    "Notamos que es de tipo _sparse matrix_, que es una forma de representar matrices poco densas. Vamos a ver la dimensión de esta matriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "213d5701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2475, 3763)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vect.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df07f879",
   "metadata": {},
   "source": [
    "Esto quiere decir que tenemos 2475 correos, y nuestro vocabulario está formado por 3763 palabras. Vamos a intentar encontrar cuáles son estas palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5740445b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '0001015',\n",
       " '01',\n",
       " '02',\n",
       " '03',\n",
       " '04',\n",
       " '05',\n",
       " '05775748',\n",
       " '057sxua1524uhkc5',\n",
       " '06',\n",
       " '07',\n",
       " '08',\n",
       " '09',\n",
       " '0rvn',\n",
       " '10',\n",
       " '100',\n",
       " '1006',\n",
       " '1008',\n",
       " '100k',\n",
       " '1012',\n",
       " '1030',\n",
       " '1046',\n",
       " '1049',\n",
       " '1052',\n",
       " '1053',\n",
       " '1054',\n",
       " '1075',\n",
       " '1090',\n",
       " '10th',\n",
       " '10x',\n",
       " '11',\n",
       " '11058',\n",
       " '111bagy1254ihzj0',\n",
       " '11956',\n",
       " '12',\n",
       " '1200bps',\n",
       " '12021',\n",
       " '12144',\n",
       " '12173',\n",
       " '12304',\n",
       " '13',\n",
       " '131',\n",
       " '133',\n",
       " '136',\n",
       " '139',\n",
       " '14',\n",
       " '141',\n",
       " '148',\n",
       " '15',\n",
       " '150',\n",
       " '153',\n",
       " '16889',\n",
       " '17',\n",
       " '17441',\n",
       " '17639',\n",
       " '179dml',\n",
       " '18',\n",
       " '180',\n",
       " '18070',\n",
       " '19',\n",
       " '1987',\n",
       " '199',\n",
       " '1rh7',\n",
       " '20',\n",
       " '200',\n",
       " '2000',\n",
       " '2002',\n",
       " '2003',\n",
       " '2004',\n",
       " '2009vjtt6',\n",
       " '200gb',\n",
       " '2022',\n",
       " '20gb',\n",
       " '21',\n",
       " '21198',\n",
       " '21st',\n",
       " '22',\n",
       " '22311',\n",
       " '225',\n",
       " '226',\n",
       " '227',\n",
       " '229',\n",
       " '23',\n",
       " '23875',\n",
       " '24',\n",
       " '24344',\n",
       " '2453',\n",
       " '24772',\n",
       " '25',\n",
       " '250',\n",
       " '256mb',\n",
       " '26',\n",
       " '261uccu3485ggcz0',\n",
       " '26792',\n",
       " '27',\n",
       " '28',\n",
       " '2800',\n",
       " '28940',\n",
       " '29',\n",
       " '30',\n",
       " '300',\n",
       " '309',\n",
       " '30k',\n",
       " '31',\n",
       " '3154',\n",
       " '32',\n",
       " '32004',\n",
       " '32053',\n",
       " '32482',\n",
       " '33',\n",
       " '34',\n",
       " '349',\n",
       " '35',\n",
       " '37',\n",
       " '3835',\n",
       " '3rd',\n",
       " '40',\n",
       " '41',\n",
       " '4179uklj5',\n",
       " '42',\n",
       " '4497vlnz2',\n",
       " '45',\n",
       " '486',\n",
       " '4863d',\n",
       " '486vnhz47',\n",
       " '49',\n",
       " '4u',\n",
       " '50',\n",
       " '500',\n",
       " '5000',\n",
       " '50_scores',\n",
       " '50mg',\n",
       " '51',\n",
       " '52',\n",
       " '5k',\n",
       " '5lbs',\n",
       " '60',\n",
       " '60_whitelist',\n",
       " '6117kfvc5',\n",
       " '6277zwyl0',\n",
       " '65000',\n",
       " '659',\n",
       " '66',\n",
       " '67',\n",
       " '68',\n",
       " '6801wbrq4',\n",
       " '6826',\n",
       " '697yfcq4277rqzm',\n",
       " '70',\n",
       " '700',\n",
       " '7099ocyt9',\n",
       " '7126kyik0',\n",
       " '7134',\n",
       " '735',\n",
       " '7377',\n",
       " '739',\n",
       " '75',\n",
       " '76',\n",
       " '77',\n",
       " '777',\n",
       " '779',\n",
       " '804',\n",
       " '805',\n",
       " '80m',\n",
       " '8119',\n",
       " '813',\n",
       " '839',\n",
       " '84',\n",
       " '840',\n",
       " '843',\n",
       " '8859',\n",
       " '886',\n",
       " '8k',\n",
       " '8simugq',\n",
       " '900',\n",
       " '9009zrld0',\n",
       " '900mhz',\n",
       " '90pre7',\n",
       " '91',\n",
       " '92',\n",
       " '940mjbj463',\n",
       " '95',\n",
       " '951',\n",
       " '97m',\n",
       " '99',\n",
       " '99_now_available',\n",
       " '_audio',\n",
       " '_me',\n",
       " '_sweet',\n",
       " '_ye',\n",
       " 'a4',\n",
       " 'a5',\n",
       " 'a52_block',\n",
       " 'a6',\n",
       " 'a6h',\n",
       " 'a7',\n",
       " 'a7a',\n",
       " 'a7k',\n",
       " 'a8',\n",
       " 'aa',\n",
       " 'aaron',\n",
       " 'about',\n",
       " 'absurdities',\n",
       " 'absurdity',\n",
       " 'abusers',\n",
       " 'access',\n",
       " 'accessibility',\n",
       " 'accessing',\n",
       " 'accident',\n",
       " 'account',\n",
       " 'accused',\n",
       " 'ack',\n",
       " 'acoustic',\n",
       " 'acquires',\n",
       " 'acronym',\n",
       " 'act',\n",
       " 'action',\n",
       " 'actions',\n",
       " 'activebuddy',\n",
       " 'activist',\n",
       " 'activity',\n",
       " 'ad',\n",
       " 'adam',\n",
       " 'adams',\n",
       " 'add',\n",
       " 'addam',\n",
       " 'added',\n",
       " 'addictive',\n",
       " 'addition',\n",
       " 'address',\n",
       " 'addresses',\n",
       " 'adhesion',\n",
       " 'admits',\n",
       " 'adn',\n",
       " 'adopts',\n",
       " 'adp',\n",
       " 'adsl',\n",
       " 'adult',\n",
       " 'adv',\n",
       " 'advance',\n",
       " 'advanced',\n",
       " 'advantage',\n",
       " 'advertising',\n",
       " 'advice',\n",
       " 'advisory',\n",
       " 'aesop',\n",
       " 'affair',\n",
       " 'afpe',\n",
       " 'afraid',\n",
       " 'africa',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'againts',\n",
       " 'age',\n",
       " 'ageing',\n",
       " 'agra',\n",
       " 'agreements',\n",
       " 'ahead',\n",
       " 'aids',\n",
       " 'aim',\n",
       " 'aint',\n",
       " 'air',\n",
       " 'ake',\n",
       " 'al',\n",
       " 'alarm',\n",
       " 'album',\n",
       " 'alert',\n",
       " 'alexander',\n",
       " 'alive',\n",
       " 'all',\n",
       " 'allen',\n",
       " 'alliance',\n",
       " 'alling',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'alphabet',\n",
       " 'already',\n",
       " 'alsa',\n",
       " 'alsaplayer',\n",
       " 'also',\n",
       " 'altivec',\n",
       " 'am',\n",
       " 'aman',\n",
       " 'amavis',\n",
       " 'amazon',\n",
       " 'amendment',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americorpsvista',\n",
       " 'ami',\n",
       " 'amorous',\n",
       " 'amphetadesk',\n",
       " 'an',\n",
       " 'analog',\n",
       " 'analysis',\n",
       " 'anarchist',\n",
       " 'anchordesk',\n",
       " 'and',\n",
       " 'angry',\n",
       " 'animated',\n",
       " 'ann',\n",
       " 'announce',\n",
       " 'announcements',\n",
       " 'announces',\n",
       " 'announcing',\n",
       " 'annual',\n",
       " 'annuity',\n",
       " 'anolther',\n",
       " 'another',\n",
       " 'answer',\n",
       " 'answers',\n",
       " 'antelope',\n",
       " 'anti',\n",
       " 'anticaipting',\n",
       " 'anticipated',\n",
       " 'any',\n",
       " 'anyone',\n",
       " 'aol',\n",
       " 'app',\n",
       " 'appeared',\n",
       " 'apple',\n",
       " 'appletfile',\n",
       " 'applicant',\n",
       " 'application',\n",
       " 'approach',\n",
       " 'approved',\n",
       " 'apps',\n",
       " 'apr',\n",
       " 'apt',\n",
       " 'aquarium',\n",
       " 'arbocks',\n",
       " 'archaeological',\n",
       " 'archive',\n",
       " 'are',\n",
       " 'aric',\n",
       " 'ark',\n",
       " 'arms',\n",
       " 'army',\n",
       " 'around',\n",
       " 'arraylist',\n",
       " 'arrays',\n",
       " 'arrested',\n",
       " 'arrived',\n",
       " 'arrrghhh',\n",
       " 'art',\n",
       " 'arthviewer',\n",
       " 'article',\n",
       " 'articles',\n",
       " 'artist',\n",
       " 'as',\n",
       " 'ascii',\n",
       " 'ashfield',\n",
       " 'aside',\n",
       " 'ass',\n",
       " 'assassin',\n",
       " 'assistance',\n",
       " 'assistant',\n",
       " 'astro',\n",
       " 'astrology',\n",
       " 'astrophysicists',\n",
       " 'asynchronous',\n",
       " 'at',\n",
       " 'atalogs',\n",
       " 'ate',\n",
       " 'atoms',\n",
       " 'att',\n",
       " 'attack',\n",
       " 'attacking',\n",
       " 'attacks',\n",
       " 'attempt',\n",
       " 'attn',\n",
       " 'auction',\n",
       " 'auctioned',\n",
       " 'auctions',\n",
       " 'audience',\n",
       " 'aug',\n",
       " 'august',\n",
       " 'ausgabe',\n",
       " 'austin',\n",
       " 'australia',\n",
       " 'australian',\n",
       " 'auth',\n",
       " 'authentication',\n",
       " 'authortracker',\n",
       " 'auto',\n",
       " 'automated',\n",
       " 'available',\n",
       " 'avast',\n",
       " 'ave',\n",
       " 'avoid',\n",
       " 'await',\n",
       " 'award',\n",
       " 'away',\n",
       " 'awl',\n",
       " 'b0_',\n",
       " 'b3',\n",
       " 'b4',\n",
       " 'b4i',\n",
       " 'b5',\n",
       " 'b5l',\n",
       " 'b6o',\n",
       " 'b7',\n",
       " 'b7q',\n",
       " 'b8',\n",
       " 'b9d',\n",
       " 'b9q',\n",
       " 'babe',\n",
       " 'baby',\n",
       " 'back',\n",
       " 'backers',\n",
       " 'backtracking',\n",
       " 'bad',\n",
       " 'badly',\n",
       " 'bags',\n",
       " 'bailout',\n",
       " 'baisley',\n",
       " 'balancing',\n",
       " 'ballet',\n",
       " 'ballmer',\n",
       " 'balloon',\n",
       " 'ban',\n",
       " 'bandit',\n",
       " 'bang',\n",
       " 'bank',\n",
       " 'bankruptcy',\n",
       " 'banned',\n",
       " 'bans',\n",
       " 'bar',\n",
       " 'barbie',\n",
       " 'barclay',\n",
       " 'bare',\n",
       " 'barnes',\n",
       " 'base',\n",
       " 'based',\n",
       " 'bass',\n",
       " 'battered',\n",
       " 'battle',\n",
       " 'battlegrounds',\n",
       " 'bay',\n",
       " 'bayesian',\n",
       " 'bazaar',\n",
       " 'bb',\n",
       " 'be',\n",
       " 'beast',\n",
       " 'beat',\n",
       " 'beats',\n",
       " 'beautiful',\n",
       " 'bebergflame',\n",
       " 'because',\n",
       " 'becomes',\n",
       " 'bed',\n",
       " 'bedbugs',\n",
       " 'beefier',\n",
       " 'been',\n",
       " 'beer',\n",
       " 'bees',\n",
       " 'before',\n",
       " 'begins',\n",
       " 'behaviours',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'belfast',\n",
       " 'believing',\n",
       " 'bell',\n",
       " 'below',\n",
       " 'belt',\n",
       " 'benefitting',\n",
       " 'bereaved',\n",
       " 'berkeley',\n",
       " 'berlinger',\n",
       " 'best',\n",
       " 'beta',\n",
       " 'betamax',\n",
       " 'betrayal',\n",
       " 'better',\n",
       " 'between',\n",
       " 'biblical',\n",
       " 'bid',\n",
       " 'big',\n",
       " 'big5',\n",
       " 'bigger',\n",
       " 'biggest',\n",
       " 'bill',\n",
       " 'billion',\n",
       " 'billions',\n",
       " 'bills',\n",
       " 'bindings',\n",
       " 'birth',\n",
       " 'birthday',\n",
       " 'biskup',\n",
       " 'bitflux',\n",
       " 'bits',\n",
       " 'bitter',\n",
       " 'biz',\n",
       " 'black',\n",
       " 'blackhole',\n",
       " 'blacklist',\n",
       " 'blackpool',\n",
       " 'blair',\n",
       " 'blamed',\n",
       " 'blazing',\n",
       " 'blew',\n",
       " 'blind',\n",
       " 'bliss',\n",
       " 'blix',\n",
       " 'blo',\n",
       " 'blog',\n",
       " 'blogging',\n",
       " 'blogs',\n",
       " 'blonds',\n",
       " 'blood',\n",
       " 'bloodhag',\n",
       " 'blow',\n",
       " 'blowout',\n",
       " 'blue',\n",
       " 'blueprint',\n",
       " 'blues',\n",
       " 'boat',\n",
       " 'body',\n",
       " 'bogofilter',\n",
       " 'bold',\n",
       " 'bomber',\n",
       " 'bond',\n",
       " 'bondage',\n",
       " 'bondedsender',\n",
       " 'bone',\n",
       " 'bones',\n",
       " 'booby',\n",
       " 'bookmobile',\n",
       " 'books',\n",
       " 'boom',\n",
       " 'boosts',\n",
       " 'boots',\n",
       " 'born',\n",
       " 'bosnia',\n",
       " 'boss',\n",
       " 'boston',\n",
       " 'bottle',\n",
       " 'bottom',\n",
       " 'bournemouth',\n",
       " 'bowers',\n",
       " 'bowling',\n",
       " 'boy',\n",
       " 'boycotting',\n",
       " 'br',\n",
       " 'brain',\n",
       " 'breaks',\n",
       " 'breakthrough',\n",
       " 'breast',\n",
       " 'breasts',\n",
       " 'brent',\n",
       " 'bridge',\n",
       " 'bridging',\n",
       " 'bright',\n",
       " 'brilliant',\n",
       " 'brings',\n",
       " 'brio',\n",
       " 'britain',\n",
       " 'british',\n",
       " 'britons',\n",
       " 'broadband',\n",
       " 'broke',\n",
       " 'broken',\n",
       " 'brown',\n",
       " 'browse',\n",
       " 'browser',\n",
       " 'browsers',\n",
       " 'bruce',\n",
       " 'bse',\n",
       " 'bsjmcoik',\n",
       " 'bskyb',\n",
       " 'bucharest',\n",
       " 'bucks',\n",
       " 'buddhaas',\n",
       " 'budget',\n",
       " 'buffalo',\n",
       " 'buffett',\n",
       " 'bug',\n",
       " 'build',\n",
       " 'building',\n",
       " 'built',\n",
       " 'bullshit',\n",
       " 'burns',\n",
       " 'bus',\n",
       " 'bush',\n",
       " 'business',\n",
       " 'bust',\n",
       " 'busy',\n",
       " 'but',\n",
       " 'butt',\n",
       " 'buy',\n",
       " 'buy4now',\n",
       " 'buys',\n",
       " 'bvax',\n",
       " 'by',\n",
       " 'bye',\n",
       " 'c1',\n",
       " 'c1qotls0tmjawm8tqnntcmcjvls00',\n",
       " 'ca',\n",
       " 'cab',\n",
       " 'cable',\n",
       " 'cache',\n",
       " 'cafe',\n",
       " 'caffeine',\n",
       " 'california',\n",
       " 'call',\n",
       " 'calling',\n",
       " 'calls',\n",
       " 'cam',\n",
       " 'cambodian',\n",
       " 'came',\n",
       " 'camera',\n",
       " 'cameras',\n",
       " 'campaign',\n",
       " 'can',\n",
       " 'canadians',\n",
       " 'cancel',\n",
       " 'cancer',\n",
       " 'canning',\n",
       " 'cannot',\n",
       " 'canon',\n",
       " 'cap',\n",
       " 'capabilities',\n",
       " 'capable',\n",
       " 'capital',\n",
       " 'capitalism',\n",
       " 'captain',\n",
       " 'capture',\n",
       " 'car',\n",
       " 'carcooning',\n",
       " 'card',\n",
       " 'career',\n",
       " 'cars',\n",
       " 'carter',\n",
       " 'cartridges',\n",
       " 'case',\n",
       " 'cases',\n",
       " 'cash',\n",
       " 'cast',\n",
       " 'catalog',\n",
       " 'catching',\n",
       " 'category',\n",
       " 'catholic',\n",
       " 'cauce',\n",
       " 'caught',\n",
       " 'causes',\n",
       " 'cave',\n",
       " 'ccaxc',\n",
       " 'cd',\n",
       " 'cds',\n",
       " 'ce',\n",
       " 'cekls',\n",
       " 'cell',\n",
       " 'cellular',\n",
       " 'census',\n",
       " 'cent',\n",
       " 'central',\n",
       " 'cents',\n",
       " 'century',\n",
       " 'cerny',\n",
       " 'cert',\n",
       " 'ceviri',\n",
       " 'cf',\n",
       " 'cfp',\n",
       " 'cgi',\n",
       " 'chairs',\n",
       " 'chakalasp',\n",
       " 'challenged',\n",
       " 'challenges',\n",
       " 'change',\n",
       " 'changed',\n",
       " 'changing',\n",
       " 'channels',\n",
       " 'charge',\n",
       " 'charges',\n",
       " 'chariots',\n",
       " 'charm',\n",
       " 'chase',\n",
       " 'chat',\n",
       " 'cheap',\n",
       " 'cheapside',\n",
       " 'cheating',\n",
       " 'check',\n",
       " 'checking',\n",
       " 'checkuser',\n",
       " 'chemical',\n",
       " 'chemically',\n",
       " 'chemistry',\n",
       " 'chess',\n",
       " 'chicago',\n",
       " 'chief',\n",
       " 'child',\n",
       " 'chimp',\n",
       " 'china',\n",
       " 'chinese',\n",
       " 'choice',\n",
       " 'choolhouserocks',\n",
       " 'choosers',\n",
       " 'christmas',\n",
       " 'cia',\n",
       " 'cigarettes',\n",
       " 'cincinnati',\n",
       " 'cio',\n",
       " 'circle',\n",
       " 'circuits',\n",
       " 'circumcision',\n",
       " 'cisco',\n",
       " 'cities',\n",
       " 'city',\n",
       " 'claim',\n",
       " 'clarification',\n",
       " 'classic',\n",
       " 'classifieds',\n",
       " 'classifier',\n",
       " 'claws',\n",
       " 'clearance',\n",
       " 'clever',\n",
       " 'click',\n",
       " 'clie',\n",
       " 'client',\n",
       " 'climate',\n",
       " 'clipping',\n",
       " 'clips',\n",
       " 'cloning',\n",
       " 'closed',\n",
       " 'closes',\n",
       " 'closures',\n",
       " 'clothespins',\n",
       " 'cloud',\n",
       " 'cloudmark',\n",
       " 'cluetrain',\n",
       " 'cnet',\n",
       " 'cnn',\n",
       " 'co2',\n",
       " 'coach',\n",
       " 'coast',\n",
       " 'cobain',\n",
       " 'cobalt',\n",
       " 'cock',\n",
       " 'code',\n",
       " 'codecs',\n",
       " 'colaco',\n",
       " 'cold',\n",
       " 'collaboration',\n",
       " 'collapse',\n",
       " 'collection',\n",
       " 'college',\n",
       " 'collision',\n",
       " 'colophon',\n",
       " 'columbine',\n",
       " 'com',\n",
       " 'combo',\n",
       " 'come',\n",
       " 'comeback',\n",
       " 'comes',\n",
       " 'coming',\n",
       " 'comma',\n",
       " 'command',\n",
       " 'comment',\n",
       " 'comments',\n",
       " 'commission',\n",
       " 'commissions',\n",
       " 'committee',\n",
       " 'communications',\n",
       " 'communism',\n",
       " 'community',\n",
       " 'comp',\n",
       " 'companies',\n",
       " 'company',\n",
       " 'compaq',\n",
       " 'compared',\n",
       " 'compatability',\n",
       " 'compatibility',\n",
       " 'compatible',\n",
       " 'compensate',\n",
       " 'compensation',\n",
       " 'compete',\n",
       " 'competition',\n",
       " 'competitive',\n",
       " 'competitor',\n",
       " 'compile',\n",
       " 'complete',\n",
       " 'complex',\n",
       " 'complicated',\n",
       " 'computational',\n",
       " 'computer',\n",
       " 'computers',\n",
       " 'computing',\n",
       " 'comrade',\n",
       " 'concept',\n",
       " 'condemned',\n",
       " 'cone',\n",
       " 'conf',\n",
       " 'conference',\n",
       " 'conferencing',\n",
       " 'confidential',\n",
       " 'configurator',\n",
       " 'configure',\n",
       " 'congo',\n",
       " 'congradulate',\n",
       " 'congratualtions',\n",
       " 'congratulations',\n",
       " 'congressman',\n",
       " 'connecting',\n",
       " 'connectivity',\n",
       " 'connery',\n",
       " 'cons',\n",
       " 'console',\n",
       " 'constraints',\n",
       " 'consult',\n",
       " 'consumer',\n",
       " 'contact',\n",
       " 'contain',\n",
       " 'content',\n",
       " 'contents',\n",
       " 'context',\n",
       " 'control',\n",
       " 'conversations',\n",
       " 'converting',\n",
       " 'conway',\n",
       " 'cool',\n",
       " 'copies',\n",
       " 'cops',\n",
       " 'copy',\n",
       " 'copying',\n",
       " 'copyright',\n",
       " 'cordless',\n",
       " 'core',\n",
       " 'corpus',\n",
       " 'corrupt_msgid',\n",
       " 'corruption',\n",
       " 'coruscate',\n",
       " 'cosmic',\n",
       " 'cost',\n",
       " 'costello',\n",
       " 'costs',\n",
       " 'costume',\n",
       " 'could',\n",
       " 'count',\n",
       " 'country',\n",
       " 'couple',\n",
       " 'court',\n",
       " 'courthouse',\n",
       " 'courts',\n",
       " 'cow',\n",
       " 'coxr',\n",
       " 'cpu',\n",
       " 'cpx',\n",
       " 'cqhcp',\n",
       " 'crack',\n",
       " 'cracking',\n",
       " 'craig',\n",
       " 'cram',\n",
       " 'cranky',\n",
       " 'crappers',\n",
       " 'crappy',\n",
       " 'crash',\n",
       " 'crazy',\n",
       " 'creaks',\n",
       " 'create',\n",
       " 'created',\n",
       " 'creative',\n",
       " 'credit',\n",
       " 'credits',\n",
       " 'crib',\n",
       " 'crime',\n",
       " 'critical',\n",
       " 'criticized',\n",
       " 'crony',\n",
       " 'crop',\n",
       " 'crosshairs',\n",
       " 'crowd',\n",
       " 'crua',\n",
       " 'crucial',\n",
       " 'cry',\n",
       " 'cryptographic',\n",
       " 'cryptography',\n",
       " 'csl',\n",
       " 'css',\n",
       " 'culture',\n",
       " 'cum',\n",
       " 'cup',\n",
       " 'cups',\n",
       " 'current',\n",
       " 'currie',\n",
       " 'curried',\n",
       " 'curse',\n",
       " 'curve',\n",
       " 'custom',\n",
       " 'customer',\n",
       " 'customers',\n",
       " 'cute',\n",
       " 'cvs',\n",
       " 'cyberage',\n",
       " 'cyberia',\n",
       " 'cyberslapps',\n",
       " 'cyberspace',\n",
       " 'cypriot',\n",
       " 'cystals',\n",
       " 'czar',\n",
       " 'd3',\n",
       " 'd9',\n",
       " 'da',\n",
       " 'dabba',\n",
       " 'daily',\n",
       " 'damage',\n",
       " 'damages',\n",
       " 'damian',\n",
       " 'dan',\n",
       " 'danger',\n",
       " 'daniel',\n",
       " 'dare',\n",
       " 'dares',\n",
       " 'dark',\n",
       " 'darkling',\n",
       " 'data',\n",
       " 'database',\n",
       " 'datapower',\n",
       " 'date',\n",
       " 'dates',\n",
       " 'dating',\n",
       " 'dave',\n",
       " 'davos',\n",
       " 'dawn',\n",
       " 'day',\n",
       " 'daypop',\n",
       " 'days',\n",
       " 'daytips',\n",
       " 'db_based_whitelist',\n",
       " 'dc',\n",
       " 'dcc',\n",
       " 'dd',\n",
       " 'de',\n",
       " 'deadlock',\n",
       " 'deal',\n",
       " 'deals',\n",
       " 'dear',\n",
       " 'death',\n",
       " 'deb',\n",
       " 'debate',\n",
       " 'debian',\n",
       " 'debianized',\n",
       " 'debt',\n",
       " 'debugging',\n",
       " 'decentralization',\n",
       " 'deck',\n",
       " 'declares',\n",
       " 'declines',\n",
       " 'deep',\n",
       " 'deeper',\n",
       " 'defaced',\n",
       " 'defaulting',\n",
       " 'defected',\n",
       " 'defences',\n",
       " 'defend',\n",
       " 'defending',\n",
       " 'defense',\n",
       " 'definitely',\n",
       " 'deflate',\n",
       " 'defnitions',\n",
       " 'dejd',\n",
       " 'delayed',\n",
       " 'delete',\n",
       " 'deleting',\n",
       " 'delias',\n",
       " 'deliver',\n",
       " 'delivers',\n",
       " 'delivery',\n",
       " 'dell',\n",
       " 'delta',\n",
       " 'democracy',\n",
       " 'denial',\n",
       " 'depend',\n",
       " 'dependencies',\n",
       " 'deployment',\n",
       " 'depth',\n",
       " 'der',\n",
       " 'deregulation',\n",
       " 'deserts',\n",
       " 'deserve',\n",
       " 'design',\n",
       " 'designed',\n",
       " 'designer',\n",
       " 'desktop',\n",
       " 'desktopengineer',\n",
       " 'detail',\n",
       " 'details',\n",
       " 'detect',\n",
       " 'detected',\n",
       " 'detective',\n",
       " 'detector',\n",
       " 'dev',\n",
       " 'devel',\n",
       " 'developer',\n",
       " 'development',\n",
       " 'device',\n",
       " 'devices',\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aecf503",
   "metadata": {},
   "source": [
    "Estas son las columnas en orden. Y ahora si pedimos un correo en particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8b0164b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train_vect[0].todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8db487",
   "metadata": {},
   "source": [
    "Como vemos, practicamente solo tenemos 0s. Vamos a ver las posicione donde hay valores no 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9346bbf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2743</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      x\n",
       "580   1\n",
       "1229  1\n",
       "2743  1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(X_train_vect[0].todense().T, columns=['x'])\n",
    "df[df['x'] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c382fc",
   "metadata": {},
   "source": [
    "Esto quiere decir que primer correo contenía solo las palabras 580, 1229 y 2743. Estas serían las siguientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "189ce80a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bug'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names()[580]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3d96e61c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'exmh'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names()[1229]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "05e6f6ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'re'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names()[2743]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2678dd1-d027-464a-87e0-6bd6e241a123",
   "metadata": {},
   "source": [
    "Ahora instanciamos nuestro modelo, que corresponde a un _Multinomial Naive Bayes_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2743a38f-e781-4ba9-a67e-b220ee5580e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584f07f0-84e6-4b36-848d-84df59ddedb9",
   "metadata": {},
   "source": [
    "Y lo entrenamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ab99f634-7318-480e-843b-03f28d03f0f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.fit(X_train_vect, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aca888a-dae7-4222-b8db-6f78a0d4fe67",
   "metadata": {},
   "source": [
    "Vamos a predecir sobre todo el _dataset_ de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cb3f381f-be44-46f5-af98-f13b7b3ea81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class = nb.predict(X_test_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecc53e3-69e6-4f8f-989b-a3532a6795dc",
   "metadata": {},
   "source": [
    "Y ahora evaluamos su _accuracy_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3519bc05-43cb-4352-bb68-023cd7e74ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9115151515151515"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d4a89f-a33b-48e1-8c86-98bea6ea75a1",
   "metadata": {},
   "source": [
    "Y ahora mostramos la matriz de confusión, que es de la forma:\n",
    "\n",
    "```\n",
    "[ TN FP\n",
    "  FN TP ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5d0c9bf5-abcf-4c1c-93bb-2b96eec0336c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[684,  39],\n",
       "       [ 34,  68]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aba2f9",
   "metadata": {},
   "source": [
    "Recordemos además que para ver la función de decisión lo podemos hacer con `predict_proba`, ya que aquí tenemos una probabilidaad, y no una función de decisión como habíamos visto hasta ahora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1ee5619f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99998298e-01, 1.70204086e-06],\n",
       "       [9.99919311e-01, 8.06888371e-05],\n",
       "       [9.99999131e-01, 8.68735424e-07],\n",
       "       ...,\n",
       "       [9.93179622e-01, 6.82037759e-03],\n",
       "       [9.27750919e-01, 7.22490807e-02],\n",
       "       [9.65845045e-01, 3.41549548e-02]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.predict_proba(X_test_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c61350c",
   "metadata": {},
   "source": [
    "Donde la probabilidad de ser spam es la segunda columna."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29180f5c",
   "metadata": {},
   "source": [
    "## Usando TF-IDF\n",
    "\n",
    "Una forma de mejorar el desempeño es usar `TfidfVectorizer`. Esta transformación hace que las palabras que sean muy comunes (ej. artículos, pronombres, ...) ponderen menos en nuestra decisión. Investigar la idea de esta transformación será parte de lo que tendrás que hacer en tu tarea. Ahora veamos cómo transformar el _dataset_ con esta técnica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "22a84cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2475x3763 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 14316 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect_tfidf = TfidfVectorizer()\n",
    "vect_tfidf.fit(X_train)\n",
    "X_train_tfidf = vect_tfidf.transform(X_train)\n",
    "X_train_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f186e49",
   "metadata": {},
   "source": [
    "Exploremos la fila que buscamos anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4bc2e924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>0.654911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>0.714966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2743</th>\n",
       "      <td>0.244776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             x\n",
       "580   0.654911\n",
       "1229  0.714966\n",
       "2743  0.244776"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(X_train_tfidf[0].todense().T, columns=['x'])\n",
    "df[df['x'] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31221dc",
   "metadata": {},
   "source": [
    "Notamos que los valores ya no son 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f8a8a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
